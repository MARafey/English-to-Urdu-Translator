{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488bde71891514d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:08:02.620638Z",
     "start_time": "2024-11-12T18:08:02.593226Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:08:02.651868Z",
     "start_time": "2024-11-12T18:08:02.629748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(directory_path):\n",
    "    # Dictionaries to store data pairs for each dataset\n",
    "    data = {\n",
    "        'train': [],\n",
    "        'dev': [],\n",
    "        'test': []\n",
    "    }\n",
    "\n",
    "    # Walk through all folders and files in the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        en_files = [f for f in files if f.endswith('.en')]\n",
    "        ur_files = [f for f in files if f.endswith('.ur')]\n",
    "\n",
    "        # Create a dictionary with matching pairs based on file names\n",
    "        paired_files = {}\n",
    "        for file in en_files:\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            paired_files[base_name] = {'en': os.path.join(root, file)}\n",
    "        \n",
    "        for file in ur_files:\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            if base_name in paired_files:\n",
    "                paired_files[base_name]['ur'] = os.path.join(root, file)\n",
    "\n",
    "        # Load contents of each paired file and separate into train, dev, and test sets\n",
    "        for base_name, paths in paired_files.items():\n",
    "            if 'en' in paths and 'ur' in paths:\n",
    "                with open(paths['en'], 'r', encoding='utf-8') as en_file, \\\n",
    "                     open(paths['ur'], 'r', encoding='utf-8') as ur_file:\n",
    "                    en_content = en_file.read().strip()\n",
    "                    ur_content = ur_file.read().strip()\n",
    "                    \n",
    "                    # Determine the set based on the file name prefix\n",
    "                    if \"train\" in base_name:\n",
    "                        data['train'].append((en_content, ur_content))\n",
    "                    elif \"dev\" in base_name:\n",
    "                        data['dev'].append((en_content, ur_content))\n",
    "                    elif \"test\" in base_name:\n",
    "                        data['test'].append((en_content, ur_content))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba03a71ffb29dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:08:03.398280Z",
     "start_time": "2024-11-12T18:08:03.369392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "directory_path = 'umc005-corpus'\n",
    "data = load_data(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19054de52b9ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:08:03.475932Z",
     "start_time": "2024-11-12T18:08:03.462500Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training pairs: {len(data['train'])}\")\n",
    "print(f\"Development pairs: {len(data['dev'])}\")\n",
    "print(f\"Test pairs: {len(data['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "754406aa3df921e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:09:10.386683Z",
     "start_time": "2024-11-12T18:09:10.338311Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "def train_tokenizers(data, prefix='tokenizer', chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizers for English and Urdu text pairs with dynamic vocabulary sizing.\n",
    "    \n",
    "    Args:\n",
    "        data: List of (english, urdu) text pairs\n",
    "        prefix: Prefix for the output model files\n",
    "        chunk_size: Number of text pairs to process in each chunk\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (english_tokenizer, urdu_tokenizer)\n",
    "    \"\"\"\n",
    "    en_texts, ur_texts = zip(*data)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir = 'tokenizer'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def save_chunked_data(texts, file_prefix):\n",
    "        chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n",
    "        file_paths = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            file_path = os.path.join(output_dir, f'{file_prefix}_chunk_{i}.txt')\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\\n\".join(chunk))\n",
    "            file_paths.append(file_path)\n",
    "        \n",
    "        return file_paths\n",
    "\n",
    "    en_files = save_chunked_data(en_texts, 'en_texts')\n",
    "    ur_files = save_chunked_data(ur_texts, 'ur_texts')\n",
    "\n",
    "    # Function to determine safe vocabulary size\n",
    "    def get_safe_vocab_size(file_path, max_allowed=5000):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        unique_tokens = len(set(text.split()))\n",
    "        # Set vocab size to minimum of unique tokens and max allowed,\n",
    "        # with a small buffer to avoid potential issues\n",
    "        return min(unique_tokens - 100, max_allowed)\n",
    "\n",
    "    en_model_paths = []\n",
    "    ur_model_paths = []\n",
    "    \n",
    "    # Train tokenizers for each chunk with appropriate vocabulary sizes\n",
    "    for i, (en_file, ur_file) in enumerate(zip(en_files, ur_files)):\n",
    "        # Calculate safe vocabulary sizes for both languages\n",
    "        en_vocab_size = get_safe_vocab_size(en_file)\n",
    "        ur_vocab_size = get_safe_vocab_size(ur_file)\n",
    "        \n",
    "        # Train English tokenizer\n",
    "        en_model_path = f'{prefix}_en_{i}.model'\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=en_file,\n",
    "            model_prefix=en_model_path.replace('.model', ''),\n",
    "            vocab_size=en_vocab_size,\n",
    "            character_coverage=1.0,\n",
    "            model_type='bpe',\n",
    "            input_sentence_size=1000000,\n",
    "            shuffle_input_sentence=True\n",
    "        )\n",
    "        en_model_paths.append(en_model_path)\n",
    "        \n",
    "        # Train Urdu tokenizer\n",
    "        ur_model_path = f'{prefix}_ur_{i}.model'\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=ur_file,\n",
    "            model_prefix=ur_model_path.replace('.model', ''),\n",
    "            vocab_size=ur_vocab_size,\n",
    "            character_coverage=0.9995,  # Higher coverage for Urdu script\n",
    "            model_type='bpe',\n",
    "            input_sentence_size=1000000,\n",
    "            shuffle_input_sentence=True\n",
    "        )\n",
    "        ur_model_paths.append(ur_model_path)\n",
    "    \n",
    "    # Load the trained models from the last chunk\n",
    "    en_tokenizer = spm.SentencePieceProcessor(model_file=en_model_paths[-1])\n",
    "    ur_tokenizer = spm.SentencePieceProcessor(model_file=ur_model_paths[-1])\n",
    "    \n",
    "    # Cleanup temporary files\n",
    "    for f in en_files + ur_files:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    \n",
    "    return en_tokenizer, ur_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "120c9d72efb8e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data` is a dictionary with 'train', 'dev', and 'test' data pairs\n",
    "en_tokenizer, ur_tokenizer = train_tokenizers(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aaea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, V)\n",
    "        return output\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear transformations and reshape\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Reshape and apply final linear transformation\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_o(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self attention\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Self attention\n",
    "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross attention\n",
    "        attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 num_layers=6,\n",
    "                 d_ff=2048,\n",
    "                 max_seq_length=5000,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Encoder and Decoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_masks(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        if src.is_cuda:\n",
    "            nopeak_mask = nopeak_mask.cuda()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.create_masks(src, tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        src_embedded = self.positional_encoding(self.encoder_embedding(src))\n",
    "        enc_output = src_embedded\n",
    "        \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt))\n",
    "        dec_output = tgt_embedded\n",
    "        \n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.final_layer(dec_output)\n",
    "        return output\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 optimizer,\n",
    "                 criterion,\n",
    "                 scheduler=None,\n",
    "                 device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        \n",
    "    def train_step(self, src, tgt):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        output = self.model(src, tgt[:, :-1])\n",
    "        loss = self.criterion(output.contiguous().view(-1, output.size(-1)),\n",
    "                            tgt[:, 1:].contiguous().view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src = src.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                \n",
    "                output = self.model(src, tgt[:, :-1])\n",
    "                loss = self.criterion(output.contiguous().view(-1, output.size(-1)),\n",
    "                                   tgt[:, 1:].contiguous().view(-1))\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_texts, target_texts, src_tokenizer, tgt_tokenizer, max_length=128):\n",
    "        self.source_texts = source_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.source_texts[idx]\n",
    "        tgt_text = self.target_texts[idx]\n",
    "        \n",
    "        # Tokenize and pad sequences\n",
    "        src_tokens = torch.tensor(\n",
    "            self.src_tokenizer.encode(src_text)[:self.max_length],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        tgt_tokens = torch.tensor(\n",
    "            self.tgt_tokenizer.encode(tgt_text)[:self.max_length],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        return src_tokens, tgt_tokens\n",
    "\n",
    "def train_transformer(train_data, val_data, src_tokenizer, tgt_tokenizer, config):\n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(\n",
    "        [pair[0] for pair in train_data],\n",
    "        [pair[1] for pair in train_data],\n",
    "        src_tokenizer,\n",
    "        tgt_tokenizer,\n",
    "        config['max_length']\n",
    "    )\n",
    "    \n",
    "    val_dataset = TranslationDataset(\n",
    "        [pair[0] for pair in val_data],\n",
    "        [pair[1] for pair in val_data],\n",
    "        src_tokenizer,\n",
    "        tgt_tokenizer,\n",
    "        config['max_length']\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_tokenizer.vocab_size(),\n",
    "        tgt_vocab_size=tgt_tokenizer.vocab_size(),\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        dropout=config['dropout']\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9\n",
    "    )\n",
    "    \n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=config['scheduler_t0'],\n",
    "        T_mult=2\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    trainer = TransformerTrainer(model, optimizer, criterion, scheduler, config['device'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src = src.to(config['device'])\n",
    "            tgt = tgt.to(config['device'])\n",
    "            \n",
    "            loss = trainer.train_step(src, tgt)\n",
    "            total_train_loss += loss\n",
    "            \n",
    "            if batch_idx % config['log_interval'] == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = trainer.evaluate(val_loader)\n",
    "        print(f'Epoch: {epoch}, Train Loss: {total_train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), config['model_path'])\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= config['early_stopping_patience']:\n",
    "            print(f'Validation loss did not improve for {config[\"early_stopping_patience\"]} epochs. Early stopping...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'max_length': 128,\n",
    "    'batch_size': 64,\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 1e-4,\n",
    "    'scheduler_t0': 10,\n",
    "    'epochs': 100,\n",
    "    'log_interval': 100,\n",
    "    'early_stopping_patience': 5,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'model_path': 'transformer.pth'\n",
    "}\n",
    "\n",
    "# Train the Transformer model\n",
    "train_transformer(data['train'], data['dev'], en_tokenizer, ur_tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize source sentence\n",
    "    src_tokens = torch.tensor(\n",
    "        src_tokenizer.encode(sentence)[:max_length],\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize target sentence with <s> token\n",
    "    tgt_tokens = torch.tensor(\n",
    "        [tgt_tokenizer.piece_to_id('<s>')],\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tokens, tgt_tokens)\n",
    "        \n",
    "        # Get most likely next token\n",
    "        next_token = output.argmax(2)[:, -1].item()\n",
    "        \n",
    "        # Append next token to target sentence\n",
    "        tgt_tokens = torch.cat((tgt_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        # Break if <\\s> token is predicted\n",
    "        if next_token == tgt_tokenizer.piece_to_id('</s>'):\n",
    "            break\n",
    "    \n",
    "    # Decode target sentence\n",
    "    translation = tgt_tokenizer.decode(tgt_tokens.squeeze().tolist())\n",
    "\n",
    "    return translation\n",
    "\n",
    "# Load the trained model\n",
    "model = Transformer(\n",
    "    src_vocab_size=en_tokenizer.vocab_size(),\n",
    "    tgt_vocab_size=ur_tokenizer.vocab_size(),\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_layers=config['num_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=config['dropout']\n",
    ")\n",
    "model.load_state_dict(torch.load(config['model_path']))\n",
    "\n",
    "# Translate a sample sentence\n",
    "sample_sentence = \"I am a student.\"\n",
    "translation = translate_sentence(sample_sentence, en_tokenizer, ur_tokenizer, model, config['device'])\n",
    "print(f\"English: {sample_sentence}\")\n",
    "print(f\"Urdu: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
